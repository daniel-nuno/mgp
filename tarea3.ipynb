{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3. Estimación MAP y Previa conjugada\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://storage.needpix.com/rsynced_images/bayesian-2889576_1280.png\" width=\"200px\" height=\"180px\" />\n",
    "\n",
    "En esta tercera tarea, reforzaremos el cálculo de estimadores MAP y de distribuciones posteriores usando previas conjugadas.\n",
    "\n",
    "Por favor, intenta ser lo más explícit@ posible, y en lo posible, apóyate de la escritura matemática con $\\LaTeX$.\n",
    "\n",
    "Recuerda además que ante cualquier duda, me puedes contactar al correo esjimenezro@iteso.mx.\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://storage.needpix.com/rsynced_images/bayesian-2889576_1280.png.</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MAP\n",
    "\n",
    "Referencia: https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php\n",
    "\n",
    "Sea $X$ una V.A. continua con la siguiente densidad:\n",
    "\n",
    "\\begin{align}\n",
    "\\nonumber p(x) = \\left\\{\n",
    "\\begin{array}{l l}\n",
    "2x & \\quad \\textrm{if }0 \\leq x \\leq 1 \\\\\n",
    "& \\quad \\\\\n",
    "0 & \\quad \\text{otherwise}\n",
    "\\end{array} \\right.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, suponga que \n",
    "\n",
    "\\begin{align}\n",
    "p(y | x) = Geometric(y | x) = (1 - x)^{y - 1} x.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enceuentre $\\hat{x}_{MAP}$ dado $Y=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estimador MAP es la función de probabilidad multiplicada por la distribución previa sobre la evidencia. Como la evidencia no depende del estimador entonces nos enfocamos en el númerador.\n",
    "\n",
    "$$ p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$$\n",
    "\n",
    "La probailidad de que Y sea 3 dado cualsea x posibles es $$ p(3|x) = (1-x)^{(3-1)}x $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de $ x \\in [0,1] $ es\n",
    "$$ \\mathcal{argmax} p(y|x)p(x) = \\mathcal{argmax} (1-x)^2x2x = 2x^2(1-x)^2$$\n",
    "\n",
    "Ahora la condición de optimalidad con al primera derivada igualada a cero.\n",
    "\n",
    "$$ \\frac{d(p(x|y))}{dx} = \\frac{d(2x^2(1-x)^2}{dx} = 2x(1-x)^2-2(1-x)x^2 = 0 $$\n",
    "$$ 2x(1-x)^2-2(1-x)x^2 = 0$$\n",
    "$$ 2x-4x^2+2x^3 - 2x^2 + 2x^3 = 2x - 6x^2 + 4x^3 $$\n",
    "\n",
    "usando la formula general $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ nos da raices en 0, 1/2 y 1,<br>\n",
    "\n",
    "la segunda derivada nos da que evaluada en 1/2 x es un máximo.\n",
    "\n",
    "$$  \\hat{x}_{MAP} = \\frac{1}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Previas conjugadas\n",
    "\n",
    "Supongamos que la función de verosimilitud es de Poisson:\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = Pois(x |\\theta) = \\frac{\\theta^x e^{-\\theta}}{x!} \n",
    "$$\n",
    "\n",
    "donde $x \\in \\mathbb{N}$. Ahora, si elegimos la distribución previa sobre el parámetro $\\theta$ como una distribución Gamma:\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\Gamma(\\theta | a, b) = \\frac{b^a}{\\Gamma(a)} \\theta^{a - 1} e^{-b \\theta}\n",
    "$$\n",
    "\n",
    "Encontrar completamente la densidad de probabilidad posterior\n",
    "\n",
    "$$\n",
    "p(\\theta | x) = \\frac{p(x | \\theta) p(\\theta)}{p(x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se le llama previa conjugada cundo la distribución posterior $ p(\\theta | x) $ es la misma distribución que la previa $ p(\\theta) $. Es decir, la distribución de la multiplicación del númerador de Bayes es la misma (función con la misma forma) a la distribución de la previa (el parámetro).\n",
    "\n",
    "Si es el caso entonces decimos que $p(\\theta)$ es conjugada para $p(x|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero calculamos la funcion de probabilidad de la distribucón de Poisson\n",
    "\n",
    "$$ l(p(x | \\theta)) = \\prod_{i=1}^{n} \\frac{\\theta^x e^{-\\theta}}{x_i!} = \\frac{\\theta^{n\\hat{x}} e^{-n\\theta}}{\\prod_{i=1}^{n}x_i!} \\propto \\theta^{n\\hat{x}} e^{-n\\theta} $$\n",
    "\n",
    "La probabilidad del paramentro $\\theta$ es aproximada a\n",
    "\n",
    "$$ p(\\theta) = \\Gamma(\\theta | \\alpha, \\beta) \\propto \\theta^{\\alpha - 1} e^{-\\beta \\theta} $$\n",
    "\n",
    "Haciendo la multiplicación de las distribuciones ignorando las partes constantes obtenemos\n",
    "\n",
    "$$ p(x | \\theta) p(\\theta) \\propto e^{-\\beta \\theta} \\theta^{\\alpha - 1} e^{-n \\theta}\\theta^{n \\hat{x}} $$\n",
    "\n",
    "$$ p(x | \\theta) p(\\theta) \\propto e^{-(\\beta + n) \\theta} \\theta^{(\\alpha +n\\hat{x}) - 1} $$\n",
    "\n",
    "Esta última expresión es corresponde con la parte no constante de una distribución gamma.\n",
    "\n",
    "$$ \\frac{p(x | \\theta) p(\\theta)}{p(x)} \\propto gamma(\\alpha+n\\hat{x}, \\beta+n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solución analítica de mínimos cuadrados y mínimos cuadrados regularizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Demuestre que la solución al problema de optimización\n",
    "\n",
    "   $$\n",
    "   \\hat{w}_{MLE} = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2,\n",
    "   $$\n",
    "\n",
    "   es $\\hat{w}_{MLE} = \\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siendo $\\phi$ la matrix de caracteristicas y $W$ el vector de coeficientes, se calcula la primera derivada como un vector gradiente, siguiendo la regla de derivación y las reglas de identidad obtenemos qué  $ \\frac{\\delta}{\\delta s} = (x-As)^T W(x-As) = -2A^TW(x-As)$\n",
    "\n",
    "$$ \\nabla_{W}f(w) = -2\\Phi^T (y-\\Phi W) = 0 $$ \n",
    "$$ \\Phi^T y = \\Phi^T \\Phi W $$\n",
    "\n",
    "Ahora, multiplicamos por la izquierda por ambos lados la matrix inversa de $(\\Phi^T\\Phi)^{-1}$\n",
    "$$ (\\Phi^T\\Phi)^{-1} \\Phi^T y = (\\Phi^T\\Phi)^{-1}(\\Phi^T \\Phi) W $$\n",
    "y reduciendo terminos nos queda,\n",
    "$$ (\\Phi^T\\Phi)^{-1} \\Phi^T y = W_{MLE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A partir de lo anterior, muestre que la solución al problema de optimización es\n",
    "\n",
    "   $$\n",
    "   \\hat{w}_{MAP} = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2 + \\lambda \\left|\\left|w\\right|\\right|^2,\n",
    "   $$\n",
    "\n",
    "   es $\\hat{w}_{MAP} = \\left(\\Phi^T \\Phi + \\lambda I \\right)^{-1} \\Phi^T y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, derivamos conforme a la regla antetior y la regla de la cadena.\n",
    "Recordando que la norma 2 $ ||W||^2 = w_1^2 + w_2^2 + ... + w_n^2 $ es igual a la multiplicación del mismo vector fila por el vector columna $W^TW$.\n",
    "\n",
    "A eso, si le agreamos una matrix identidad es útil cuando queremos multiplicar por un escalar ($\\lambda$)  $W^T I W$\n",
    "\n",
    "$$ \\nabla_{W} ||y-\\Phi W||^2 = \\nabla_w (y - \\Phi W)^T I (y-\\Phi W) $$\n",
    "$$ \\nabla_{W} \\lambda||W||^2 = 2\\lambda W $$\n",
    "\n",
    "\n",
    "$$ \\nabla_{W}f(w) = -2\\Phi^T (y-\\Phi W) + 2\\lambda W = 0 $$\n",
    "$$ \\nabla_{W}f(w) = -\\Phi^Ty + \\Phi^T\\Phi W + \\lambda W = 0 $$\n",
    "Restando $\\lambda W$ de ambos lados\n",
    "$$ \\nabla_{W}f(w) = -\\Phi^Ty + \\Phi^T\\Phi W  + \\lambda W -\\lambda W = -\\lambda W $$\n",
    "\n",
    "Después de restar $\\Phi^T\\Phi W)$ de ambos lados\n",
    "$$ \\nabla_{W}f(w) = -\\Phi^Ty = -\\lambda W - \\Phi^T\\Phi W $$\n",
    "\n",
    "Después factorizando W y introdución una matrix identidad para $\\lambda$\n",
    "$$ \\nabla_{W}f(w) = -\\Phi^Ty = (\\Phi^T\\Phi + \\lambda I)W $$\n",
    "\n",
    "Después de multiplicar por la inversa de $(\\Phi^T\\Phi + \\lambda I)$\n",
    "$$ \\nabla_{W}f(w) = -\\Phi^Ty(\\Phi^T\\Phi + \\lambda I)^{-1} = W_{MAP} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
